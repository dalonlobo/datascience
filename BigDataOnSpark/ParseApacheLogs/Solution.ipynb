{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialising spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re # Using re module for regular expressions\n",
    "\n",
    "from pyspark.sql import SparkSession # Importing SparkSession "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialising sparkContext\n",
    "spark = SparkSession.builder.appName(\"LogFileParser\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Path where the log file is located\n",
    "filePath = \"/user/dalonlobo2857/Spark\"\n",
    "\n",
    "# Read the log file using sc.textFile and store the rdd in logFileRdd\n",
    "logFileRdd = sc.textFile(filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'in24.inetnebr.com - - [01/Aug/1995:00:00:01 -0400] \"GET /shuttle/missions/sts-68/news/sts-68-mcc-05.txt HTTP/1.0\" 200 1839',\n",
       " u'uplherc.upl.com - - [01/Aug/1995:00:00:07 -0400] \"GET / HTTP/1.0\" 304 0']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the log file is read correctly\n",
    "logFileRdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer to Q1\n",
    "\n",
    "To find out top 10 requested URLs along with count of number of times they have been requested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Based on the pattern of log file entry, I've written the following regular expression\n",
    "# This will break the log entry in to groups and then we can use those groups to solve the problem\n",
    "# Each group of regular expression is explained below in the comments.\n",
    "\n",
    "regex = r\"\"\"(?:[\\w\\-]*(?=[/,@:&]))?[/,@:&]? # Used to cleanup the domain name/host name/ip\n",
    "            ([\\w\\.\\-\\*]+)                   # First group with domain name/host name/ip (Group 1)\n",
    "            (?:\\s-){2}\\s                    # Unwanted characters\n",
    "            \\[(.*)\\]                        # Second group with timestamp               (Group 2)\n",
    "            \\s\"                             # Space and \", these are consumed by regex\n",
    "            ([\\w]+)?\\s*                     # Request type, ex: GET, POST               (Group 3)\n",
    "            ([^\\s]+)?\\s*                    # Requested url                             (Group 4)\n",
    "            (.*(?=[\\s*][HTTP]))?\\s*         # Unwanted characters                       (Group 5)\n",
    "            (HTTP/\\d\\.\\d)?                  # Request prototype and version             (Group 6)\n",
    "            (.*)?                           # Unwanted characters                       (Group 7)\n",
    "            \"\\s                             # \" and space\n",
    "            (\\d+)\\s                         # HTTP reply code                           (Group 8)\n",
    "            ([\\d\\-]+)                       # Number of bytes returned by the server    (Group 9)\n",
    "        \"\"\"\n",
    "\n",
    "# The following function will split the lines based on the regular expression\n",
    "def splitLines(line):\n",
    "    try:\n",
    "        return (re.match(regex, line, re.VERBOSE).groups()[3], 1)\n",
    "    except Exception as e:\n",
    "        return ('Regex match failed', line)\n",
    "\n",
    "# Using map to apply 'splitLines' function on each line\n",
    "# splitLines will return a tuple with Requested URL as key and count as value\n",
    "urlCountRdd = logFileRdd.map(splitLines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Using reduceByKey function, since each entry in rdd is key = URL and Value = count\n",
    "# reduceByKey function will do the aggregation \n",
    "reducedUrlCountRdd = urlCountRdd.reduceByKey(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The supplied regular expression has matched all the entries successfully\n"
     ]
    }
   ],
   "source": [
    "# Testing if any record in rdd has failed to match the regular expression\n",
    "if reducedUrlCountRdd.filter(lambda x: x[0] == \"Regex match failed\").collect() == []:\n",
    "    print(\"The supplied regular expression has matched all the entries successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to format the ouput\n",
    "def formatOutput(key, val):\n",
    "    key = key.ljust(40)\n",
    "    val = str(val).center(11)\n",
    "    return (key, val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result: Top 10 requested URLs along with count of number of times they have been requested is displayed below.\n",
    "\n",
    "I will ignore URL '/', because this URL will not give us more information on the page requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Rank---|------------------URL-------------------|---Count---\n",
      "----1-----|/images/NASA-logosmall.gif              |   97410   \n",
      "----2-----|/images/KSC-logosmall.gif               |   75337   \n",
      "----3-----|/images/MOSAIC-logosmall.gif            |   67448   \n",
      "----4-----|/images/USA-logosmall.gif               |   67068   \n",
      "----5-----|/images/WORLD-logosmall.gif             |   66444   \n",
      "----6-----|/images/ksclogo-medium.gif              |   62778   \n",
      "----7-----|/ksc.html                               |   43687   \n",
      "----8-----|/history/apollo/images/apollo-logo1.gif |   37826   \n",
      "----9-----|/images/launch-logo.gif                 |   35138   \n",
      "----10----|/                                       |   30347   \n",
      "----11----|/images/ksclogosmall.gif                |   27810   \n"
     ]
    }
   ],
   "source": [
    "# Top 10 requested URLs are displayed below, displaying top 11 URLs including /:\n",
    "print(\"{0}|{1}|{2}\".format(\"Rank\".center(10, '-'), \n",
    "                           \"URL\".center(40, '-'), \n",
    "                           \"Count\".center(11, '-')))\n",
    "\n",
    "for item, element in enumerate(reducedUrlCountRdd.takeOrdered(11, key=lambda x: -x[1])):\n",
    "    print(\"{0}|{1}|{2}\".format(str(item + 1).center(10, '-'),\n",
    "                               *formatOutput(element[0], element[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Insights\n",
    "> - Most of the requested URLs are of .gif files\n",
    "> - Among html files ksc.html is requested 43687 times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer to Q2\n",
    "\n",
    "Spark code to find out top 5 hosts/IP making the request along with count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# All the imports\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# import datatypes for schema\n",
    "from pyspark.sql.types import StringType, StructType, StructField, TimestampType\n",
    "from pyspark.sql.functions import desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Based on the pattern of log file entry, I've written the following regular expression after many iterations\n",
    "# This will break the log entry in to groups and then we can use those groups to solve the problem\n",
    "# Each group of regular expression is explained below in the comments.\n",
    "\n",
    "# Ignoring the timezone -0400 since all the entries are in same timezone\n",
    "\n",
    "regex = r\"\"\"(?:[\\w\\-]*(?=[/,@:&]))?[/,@:&]? # Used to cleanup the domain name/host name/ip\n",
    "            ([\\w\\.\\-\\*]+)                   # First group with domain name/host name/ip (Group 1)\n",
    "            (?:\\s-){2}\\s                    # Unwanted characters\n",
    "            \\[(.*)\\s-0400\\]                 # Second group with timestamp               (Group 2)\n",
    "            \\s\"                             # Space and \", these are consumed by regex\n",
    "            ([\\w]+)?\\s*                     # Request type, ex: GET, POST               (Group 3)\n",
    "            ([^\\s]+)?\\s*                    # Requested url                             (Group 4)\n",
    "            (.*(?=[\\s*][HTTP]))?\\s*         # Unwanted characters                       (Group 5)\n",
    "            (HTTP/\\d\\.\\d)?                  # Request prototype and version             (Group 6)\n",
    "            (.*)?                           # Unwanted characters                       (Group 7)\n",
    "            \"\\s                             # \" and space\n",
    "            (\\d+)\\s                         # HTTP reply code                           (Group 8)\n",
    "            ([\\d\\-]+)                       # Number of bytes returned by the server    (Group 9)\n",
    "        \"\"\"\n",
    "\n",
    "# The following function will split the lines based on the regular expression\n",
    "def splitLines(line):\n",
    "    try:\n",
    "        tem = re.match(regex, line, re.VERBOSE).groups()\n",
    "        return [tem[0], datetime.strptime(tem[1], '%d/%b/%Y:%H:%M:%S'), tem[3], tem[7], tem[8]]\n",
    "    except Exception as e:\n",
    "        return ['Regex match failed', e]\n",
    "\n",
    "# Using map to apply 'splitLines' function on each line\n",
    "# splitLines will return a list with all the required matched groups\n",
    "generalRdd = logFileRdd.map(splitLines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'in24.inetnebr.com',\n",
       "  datetime.datetime(1995, 8, 1, 0, 0, 1),\n",
       "  u'/shuttle/missions/sts-68/news/sts-68-mcc-05.txt',\n",
       "  u'200',\n",
       "  u'1839']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display one element of rdd\n",
    "generalRdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Converting rdd to dataframe for ease of use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converting the rdd to df\n",
    "\n",
    "fields = [StructField(\"HostName\", StringType()),\n",
    "         StructField(\"TimeStamp\", TimestampType()),\n",
    "         StructField(\"URL\", StringType()),\n",
    "         StructField(\"ResponseCode\", StringType()),\n",
    "         StructField(\"Bytes\", StringType())]\n",
    "\n",
    "schema = StructType(fields)\n",
    "\n",
    "logDf = generalRdd.toDF(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------------------+------------+-----+\n",
      "|         HostName|           TimeStamp|                 URL|ResponseCode|Bytes|\n",
      "+-----------------+--------------------+--------------------+------------+-----+\n",
      "|in24.inetnebr.com|1995-08-01 00:00:...|/shuttle/missions...|         200| 1839|\n",
      "|  uplherc.upl.com|1995-08-01 00:00:...|                   /|         304|    0|\n",
      "|  uplherc.upl.com|1995-08-01 00:00:...|/images/ksclogo-m...|         304|    0|\n",
      "|  uplherc.upl.com|1995-08-01 00:00:...|/images/MOSAIC-lo...|         304|    0|\n",
      "|  uplherc.upl.com|1995-08-01 00:00:...|/images/USA-logos...|         304|    0|\n",
      "+-----------------+--------------------+--------------------+------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the values in data frame\n",
    "logDf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "groupByHostNameDF = logDf.groupBy(logDf.HostName).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result: Top 5 hosts/IP making the request along with count is displayed below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            HostName|count|\n",
      "+--------------------+-----+\n",
      "|  edams.ksc.nasa.gov| 6530|\n",
      "|piweba4y.prodigy.com| 4846|\n",
      "|        163.206.89.4| 4791|\n",
      "|piweba5y.prodigy.com| 4607|\n",
      "|piweba3y.prodigy.com| 4416|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "groupByHostNameDF.orderBy(desc(\"count\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Insights\n",
    "> Server got 6530 requests from edams.ksc.nasa.gov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer to Q3\n",
    "\n",
    "Spark code to find out top 5 time frame for high traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format, udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creating a new column, which contain TimeFrame\n",
    "timeFramelogDf = logDf.withColumn(\"TimeFrame\", date_format(logDf[\"TimeStamp\"], \"dd/MM/YYYY:HH\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "groupByTimeFrameDf = timeFramelogDf.groupBy(timeFramelogDf.TimeFrame).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result: Top 5 time frame of highest traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|    TimeFrame|count|\n",
      "+-------------+-----+\n",
      "|31/08/1995:11| 6321|\n",
      "|31/08/1995:10| 6283|\n",
      "|31/08/1995:13| 5948|\n",
      "|30/08/1995:15| 5919|\n",
      "|31/08/1995:09| 5627|\n",
      "+-------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "timeFrameOrderDesc = groupByTimeFrameDf.orderBy(desc(\"count\"))\n",
    "timeFrameOrderDesc.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Insights\n",
    "> - Top 5 time frames with highest traffic are displayed above\n",
    "> - On 31/08/1995 at 11 hours, the server received highest traffic of 6321 requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping by day of week and hour of day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# User defined function\n",
    "formatTimestamp = udf(lambda x: x.strftime(\"%A %H\"), StringType())\n",
    "\n",
    "# Creating a new column, which contains TimeFrame\n",
    "weekdayHourlogDf = logDf.withColumn(\"WeekdayHour\", formatTimestamp(logDf[\"TimeStamp\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|WeekdayHour|count|\n",
      "+-----------+-----+\n",
      "|Thursday 15|23380|\n",
      "|Thursday 12|23035|\n",
      "| Tuesday 13|21115|\n",
      "| Tuesday 12|20908|\n",
      "|Thursday 13|20423|\n",
      "+-----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weekdayHourlogDf.groupBy(weekdayHourlogDf.WeekdayHour)\\\n",
    "                .count()\\\n",
    "                .orderBy(desc(\"count\"))\\\n",
    "                .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Insights\n",
    "> The company is receiving peak traffic on Thursday's at 15 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer to Q4\n",
    "\n",
    "Spark code to find 5 time frames of least traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I will reuse groupByTimeFrameDf from Q3\n",
    "timeFrameOrderAsc = groupByTimeFrameDf.orderBy(\"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result: Top 5 time frame of highest traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|    TimeFrame|count|\n",
      "+-------------+-----+\n",
      "|03/08/1995:04|   16|\n",
      "|03/08/1995:09|   22|\n",
      "|03/08/1995:05|   43|\n",
      "|03/08/1995:10|   57|\n",
      "|03/08/1995:07|   58|\n",
      "+-------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "timeFrameOrderAsc.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Insights\n",
    "> - 5 time frames with least traffic are displayed above\n",
    "> - On 03/08/1995 at 04 hours, the server received least traffic of only 16 requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping by day of week and hour of day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|WeekdayHour|count|\n",
      "+-----------+-----+\n",
      "|  Sunday 06| 2437|\n",
      "|Saturday 05| 2579|\n",
      "|  Sunday 05| 2734|\n",
      "|Saturday 06| 2748|\n",
      "|  Sunday 04| 2807|\n",
      "+-----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using the weekdayHourlogDf from Q3\n",
    "weekdayHourlogDf.groupBy(weekdayHourlogDf.WeekdayHour)\\\n",
    "                .count()\\\n",
    "                .orderBy(\"count\")\\\n",
    "                .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Insights\n",
    "> The company can do production deployment on Sunday's at 06 hours as the servers are least used at that time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer to Q5\n",
    "\n",
    "Spark code to find out unique HTTP codes returned by the server along with count.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using logDF\n",
    "groupByRespCodeDF = logDf.groupBy(logDf.ResponseCode).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result: Unique HTTP codes returned by the server along with count is displayed below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Response Code----|-----Count-----\n",
      "        200         |    1398988\n",
      "        304         |     134146\n",
      "        302         |      26497\n",
      "        404         |      10056\n",
      "        403         |        171\n",
      "        501         |         27\n",
      "        400         |         10\n",
      "        500         |          3\n"
     ]
    }
   ],
   "source": [
    "# Formatting the result\n",
    "print(\"{0}|{1}\".format(\"Response Code\".center(20, '-'), \n",
    "                           \"Count\".center(15, '-')))\n",
    "\n",
    "for row in groupByRespCodeDF.orderBy(desc(\"count\")).collect():\n",
    "    print(\"{0}|{1}\".format(str(row.ResponseCode).center(20, ' '),\n",
    "                           str(row[\"count\"]).rjust(11, ' ')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Insights\n",
    "> - Most of the requests got response code of 200\n",
    "> - Only 3 internal server errors i.e. response code 500"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
